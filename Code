####TOPIC MODELING ####
#1 Transform reviews into term-document matrix, lemmatize, remove stop words, set minimal document frequency
import nltk
import pandas as pd
data= pd.read_excel("/Users/meghanharris/Desktop/Assignment 2 text.xlsx")
datalist = []

for i in data["review"]:
    datalist.append(i)

datalist1=[]
for j in datalist:
    tokenized_reviews=nltk.word_tokenize(j.lower())
    datalist1.append(tokenized_reviews)

stop_words_removed=[]
stop_words= nltk.corpus.stopwords.words("english")
for sentence in datalist1:
    temp2=[]
    for word in sentence:
        if word not in stop_words and word.isalpha():
            temp2.append(word)
    stop_words_removed.append(temp2)


lemmatizer=nltk.stem.WordNetLemmatizer()
lemmatized=[]
for sentence in stop_words_removed:
    temp1=[]
    for word in sentence:
        temp1.append(lemmatizer.lemmatize(word))
        if word == "quot" or word == "gt" or word== "lt":
            temp1.remove(word)
    lemmatized.append(temp1)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer= TfidfVectorizer(ngram_range=(1,2), min_df=5)
datalist2=[]

for i in lemmatized:
    datalist2.append(" ".join(i))

vectorizer.fit(datalist2)
v1=vectorizer.transform(datalist2)

print(v1)

#2 Using LDA model to extract 6 topics of ech document 
from sklearn.feature_extraction.text import CountVectorizer
vectorizer7= CountVectorizer(stop_words="english")
x2= vectorizer7.fit_transform(datalist2)
terms=vectorizer7.get_feature_names()

from sklearn.decomposition import LatentDirichletAllocation
lda=LatentDirichletAllocation(n_components=6).fit(x2)

for topic_idx, topic in enumerate(lda.components_):
    print("Topic %d:" % (topic_idx))
    print(" ".join([terms[i] for i in topic.argsort()[:-4-1:-1]]))

#3 Reporting topic distribution of the top 2 topics of the first 10 restaurant reviews and first 10 movie reviews
#topic distribution
lda.transform(x2[1])
#print(lda.components_)

#Top 2 topics for each restaurant review
doc_topic_distrib_res = pd.DataFrame(lda.transform(x2[1:11,]), index=range(10),columns= ["Topic 0", "Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5"]).T
print(doc_topic_distrib_res)

Tops_Rest= pd.DataFrame(doc_topic_distrib_res.T.apply(lambda x:list(doc_topic_distrib_res.columns[np.array(x).argsort()[::-1][:2]]), axis=1).to_list(), columns=["Top1","Top2"])
print(Tops_Rest)

#Top 2 topics for each movie review
doc_topic_distrib_movie = pd.DataFrame(lda.transform(x2[502:512,]), columns= ["Topic 0", "Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5"]).T
print(doc_topic_distrib_movie)

Tops_Movie= pd.DataFrame(doc_topic_distrib_movie.T.apply(lambda x:list(doc_topic_distrib_movie.columns[np.array(x).argsort()[::-1][:2]]), axis=1).to_list(), columns=["Top1","Top2"])
print(Tops_Movie)

#4 Finding the top 5 terms for each 6 topics

for index, topic in enumerate(lda.components_):
    print(f'Top 5 words for Topic {index}')
    print([vectorizer7.get_feature_names()[i] for i in topic.argsort()[-5:]])
